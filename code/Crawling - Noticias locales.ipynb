{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crawling - Noticias locales.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oKwC_xmpkVsB","colab_type":"text"},"source":["# Crawling - Scrapy para obtener datos de noticias locales de Madrid\n","\n","La dirección de noticias locales de Madrid que voy a utilizar es: https://www.eldistrito.es/distritos/\n","Esta dirección me viene muy bien porque tiene las noticas separadas por distritos que posteriormente podré asociar a barrios en los que están las viviendas de Airbnb\n","\n","Con este Scrapy consigo un csv con los siguientes datos: fecha de la noticia, distrito y titular de la noticia.\n","\n","En principio para no \"machacar\" mucho la página solo estoy accediendo a 4 páginas de noticias que son un total de 32 noticas"]},{"cell_type":"code","metadata":{"id":"HVnhcsucksBM","colab_type":"code","colab":{}},"source":["!pip install scrapy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdSIHs98nJ9l","colab_type":"code","colab":{}},"source":["import scrapy\n","import json\n","\n","class NoticiasItem(scrapy.Item):\n","    distrito = scrapy.Field()\n","    titular = scrapy.Field()\n","    fecha = scrapy.Field()\n","\n","class DistritoSpider(scrapy.Spider):\n","    name = 'distritospider'\n","    # URL Inicial\n","    start_urls = ['https://www.eldistrito.es/distritos/']\n","    \n","    def parse(self, response):\n","        \n","        for noticia in response.css('div.noticiaportada'):\n","            item = NoticiasItem ()\n","            #Me voy a quedar con el distrito y el titular de la noticia\n","            item['distrito'] = noticia.css('div.ruta2_sin a::text').extract()[-1]\n","            item ['titular'] = noticia.css('h1.titular a b::text').extract_first() \n","            \n","            #Me interesa también la fecha de la noticia, para eso hay que entrar en el link de la noticia\n","            pagina_noticia = noticia.css ('h1.titular a::attr(href)').extract_first()\n","            yield response.follow(pagina_noticia, meta ={\"item\":item}, callback=self.parse_noticia)\n","            \n","            \n","       \n","        for next_page in response.css ('td.ant_sig a'):\n","            #Hay 2 href, el primero va hacia la página anterior y el segundo hacia la siguiente por eso me quedo\n","            #con el último [-1]\n","            href = response.css ('td.ant_sig a::attr(href)').extract()[-1]\n","            if href is not None:\n","                #Para no hacer mucho daño me quedo solo con 3 páginas de noticias, van de 8 en 8\n","                #Para un proyecto final podemos quitar esta condición y traernos todo\n","                if int(href.split('/')[-1]) > 24:\n","                    return\n","            \n","                yield response.follow(href, self.parse)\n","                \n","    def parse_noticia (self, response): \n","        #Dentro de la noticia para obtener la fecha\n","        item = response.meta [\"item\"]\n","        item['fecha'] = response.css ('span.fecha::text').extract_first()\n","        f=item['fecha']\n","        d=item['distrito']\n","        t=item['titular']\n","        print(f\"{f};{d}; {t}\", file=filep)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFssuLo9oR3F","colab_type":"code","colab":{}},"source":["from scrapy.crawler import CrawlerProcess\n","\n","\n","process = CrawlerProcess({\n","    'USER_AGENT': 'Crawler test - formacion bootcamp bigdata'\n","})\n","\n","filep = open('datanoticias.csv', 'w')\n","print(\"{};{};{}\".format(\"fecha\", \"distrito\", \"titular\"), file=filep)\n","process.crawl(DistritoSpider)\n","process.start()\n","filep.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gi9QZRfuo0Uy","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download('datanoticias.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0ftn2eEvCZg","colab_type":"text"},"source":["**Con el siguiente código podemos cargar directamente el fichero en el Google Storage**"]},{"cell_type":"code","metadata":{"id":"9gXRP8GVvKTG","colab_type":"code","colab":{}},"source":["from google.colab import auth\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f72710onvNy-","colab_type":"code","colab":{}},"source":["from googleapiclient.discovery import build\n","gcs_service = build('storage', 'v1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFPxCtbivSNm","colab_type":"code","colab":{}},"source":["from googleapiclient.http import MediaFileUpload\n","\n","media = MediaFileUpload('datanoticias.csv', \n","                        mimetype='text/plain',\n","                        resumable=True)\n","\n","request = gcs_service.objects().insert(bucket='dataproc-fe7b85fd-43dc-4de0-a520-a824fcd432de-europe-west1', \n","                                       name='datanoticias.csv',\n","                                       media_body=media)\n","\n","response = None\n","while response is None:\n","  # _ is a placeholder for a progress object that we ignore.\n","  # (Our file is small, so we skip reporting progress.)\n","  _, response = request.next_chunk()\n","\n","print('Upload complete')"],"execution_count":0,"outputs":[]}]}